{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Enterprise Event Bus","text":""},{"location":"#what-is-the-enterprise-event-bus","title":"What is the Enterprise Event Bus","text":"<p>The Enterprise Event Bus (EEB) is a centralized, scalable platform that enables secure, real-time data sharing across VA systems and applications. Unlike a single purpose solution, the Event Bus is designed to unify events from both healthcare and benefits, making them accessible to a wide range of products and teams across VA. By leveraging an event-driven architecture, the Event Bus allows product teams to:</p> <ul> <li>Share real time data: Deliver critical updates and build more seamless experiences for end-users.</li> <li>Integrate systems enterprise wide: Connect to disparate systems across healthcare and benefits, reducing operational silos.</li> <li>Ensure reliability: Prevent silent failures and maintain data integrity with built-in safeguards and monitoring. </li> </ul> <p>The Event Bus is the foundation for creating a connected, integrated VA ecosystem that empowers teams to deliver better outcomes for Veterans and staff while the Event Bus handles reliable, secure data distribution at scale. </p>"},{"location":"#why-use-the-enterprise-event-bus","title":"Why use the Enterprise Event Bus","text":"<p>The Enterprise Event Bus delivers value to Veterans, VA staff, and the VA organization by addressing key challenges and unlocking new opportunities:</p> <p>Improves Veteran and Staff Experiences</p> <ul> <li>Ensures Veterans receive coordinated, timely, and accurate updates across all their interactions with VA.</li> <li>Eliminates duplicate data entries so that Veterans and staff don\u2019t have to enter information repeatedly to get pertinent information immediately.</li> <li>Ensures critical updates flow seamlessly across systems.</li> </ul> <p>Accelerates Delivery</p> <ul> <li>Cuts integration time through standardized connection pattern for integrating with systems.</li> <li>Teams can concentrate on what they do best without needing to understand how other systems work.</li> <li>Replaces \u201cyour turn, my turn\u201d development with a model where all teams can build at the same time.</li> </ul> <p>Drives Operational Efficiency</p> <ul> <li>Breaks down silos by enabling seamless data sharing between healthcare and benefits systems.</li> <li>Supports smarter use of resources by lowering the cost and complexity of system integrations.</li> </ul> <p>Enables Enterprise-Scale Resilience</p> <ul> <li>Handles high volumes of events across multiple mission critical domains, ensuring VA can scale to meet future demands.</li> <li>Strengthens system resilience by preventing cascading failures when individual components experience issues. </li> <li>Provides transparency through real-time monitoring, reporting, and system health visibility.</li> </ul>"},{"location":"#determining-if-the-event-bus-is-right-for-you","title":"Determining if the Event Bus is Right for You","text":"<p>Before you make further plans to leverage enterprise events, you should evaluate if event-driven architecture is the right fit for your system. Please read the content below, which describes the sorts of systems that would benefit from enterprise events.</p> Event Bus is a Good Fit if... Event Bus is Not a Good Fit if...                  Your system has multiple tightly connected parts that need to communicate frequently and independently. Events help decouple these parts so the system stays responsive even if one component is delayed or updated.                 Example: A benefits system where claims processing, appointment scheduling, eligibility checks, pharmacy orders, and referrals all interact but evolve separately.                  Your system is simple, changes frequently, or has loosely defined interactions.                 Example: A form that collects patient feedback, or a prototype platform for testing various intake workflows.                  You want to eliminate inefficient polling and process high volumes of data as it arrives. Events allow real-time updates without delays.                 Example: A care coordination dashboard that reflects provider notes instantly, or a claims platform that processes submissions in real time instead of in overnight batches.                  Your system checks data infrequently or only needs to run on a schedule.                 Example: A tool that retrieves a Veteran's last lab result, or a monthly report on healthcare usage trends.                  You need to send real-time updates to users.                 Example: A Veteran gets an instant alert when their benefit status changes or an appointment is confirmed.                  Timeliness isn't critical and users can wait for updates.                 Example: Notifications about annual benefits re-enrollment.                  You want parts of your system to work independently. Events allow them to update each other without waiting.                 Example: A pharmacy system that queues refill requests and continues processing others while waiting for prescription verification to complete.                  Your system relies on synchronous calls between services that wait for a response.                 Example: A benefit calculator that waits for real-time income validation.                  Your data is Low or Moderate sensitivity as rated by FISMA/VA system categorization.                               Your data is High sensitivity as rated by FISMA/VA system categorization.              <p>Learn more about FISMA. </p> <p>Learn more about VA system categorization (must be part of VA GitHub organization to view).</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Asynchronous real-time processing ensures fast, efficient, and scalable operations by handling multiple tasks simultaneously without delays. This means your applications can process live data, respond instantly to user interactions, and seamlessly scale to handle more users without slowdowns or bottlenecks.</li> <li>Decoupled communication through an Event Bus allows system components to interact independently and react to data changes in near real-time. This improves flexibility, scalability, and resilience by reducing dependencies, enabling faster innovation, minimizing downtime, and eliminating the need for batch processing or scheduled data pulls.</li> <li>Event Bus offers easy integration with existing systems. Systems can connect to standard tools like Kafka clients, making it easier to send and receive data without changing how they already work.</li> <li>Event Bus functions as a centralized hub for events. By serving as a shared infrastructure for event-driven communication across VA, it allows systems to exchange both healthcare and benefit data.</li> <li>Built on Apache Kafka providing strong reliability. The Event Bus ensures high availability, durability, and fault tolerance through features like message replication, persistent storage, and configurable delivery guarantees.</li> </ul>"},{"location":"#how-it-works","title":"How it Works","text":""},{"location":"#how-events-flow-through-the-system","title":"How Events Flow Through the System","text":"<ul> <li>The Event Bus simplifies how teams share and react to data changes across systems. Here\u2019s how it works: </li> <li>Event Producers: Systems generate events based on state changes in the data and publish them to the Event Bus<ul> <li>Key Benefit: Event producers don't need to know who the interested consumers are and therefore don't need to keep growing their outbound complexity.</li> </ul> </li> <li>Event Consumers: Any system can subscribe to these events and take action accordingly<ul> <li>Key Benefit: Multiple consumers may receive and act on an event; it's not just an asynchronous queue sitting between a single producer and consumer.</li> </ul> </li> </ul>"},{"location":"#event-bus-workflow-example","title":"Event Bus Workflow Example","text":"<p>Here is an example of how Event Bus hypothetically could work:</p> <p>When a Veteran visits their doctor and receives a new prescription, the system publishes an event to the Event Bus. This event instantly updates the local VA clinic\u2019s records for reference for future appointments, notifies the pharmacy to prepare the medication, and sends an SMS to the Veteran letting them know their documents have been updated with their local VA clinic \u2014 all at the same time, without manual coordination.</p> <p></p> <p></p> <p></p>"},{"location":"#active-use-case-veterans-notified-of-claims-much-earlier-with-event-bus","title":"Active Use Case: Veterans Notified of Claims Much Earlier with Event Bus","text":"<p>Challenge: After a Veteran files a disability claim, they must wait until they receive their Decision Letter in the mail to learn the outcome, delaying their ability to plan appeals or provide additional information </p> <p>Solution: A team that owns the Claims Status Tool on va.gov and the VA Health &amp; Benefits app within the OCTO Benefits Portfolio has partnered with the Event Bus to \"listen\" for these Decision Letter Availability Events. </p> <ul> <li>When a Decision Letter is generated, the Event Bus triggers an immediate email notification to the Veteran, letting them know they can view the letter in their eFolder. </li> </ul> <p>Impact: Veterans are now notified 7-10 days earlier, giving them more time to plan appeals or satisfy requests for additional information while also reducing uncertainty during the claims process. </p> <p>This feature went live in production in late June 2025. It currently sends about 95K emails a week and has a click-through rate greater than 50%.</p> <p></p> <p></p> <p></p>"},{"location":"#event-bus-introduction","title":"Event Bus Introduction","text":"<p>Watch this video introduction to Event Bus to learn more about how it works:</p>"},{"location":"#event-bus-visual-demonstration","title":"Event Bus Visual Demonstration","text":"<p>For a more technical breakdown, there is also this video from August 2023 about the state of the system at that time.</p> <p>Here is a visual demo to help show Event Bus in action:</p> <p>Learn how to produce events Learn how to consume events</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#next-steps","title":"Next Steps","text":"<p>Take the next steps to explore and begin using the Enterprise Event Bus:</p> <ul> <li>Understand the administrative requirements for connecting to the Enterprise Event Bus,</li> <li>Dive deeper into a technical explanation of events,</li> <li>Learn how to produce events,</li> <li>Learn how to consume events, or </li> <li>Reach out to us with questions.</li> </ul>"},{"location":"#reach-out-to-us","title":"Reach out to us","text":"<p>If you think the Enterprise Event Bus would be a good fit for your situation, please see our Contact and Support page!</p> <p>The best way to contact the Event Bus Team is via the #ves-event-bus Slack channel (must be in the Office of CTO @VA workspace).</p>"},{"location":"administrative-requirements/","title":"Start Guide and Administrative Requirements","text":""},{"location":"administrative-requirements/#start-guide","title":"Start Guide","text":"<p>This guide covers everything you need to integrate with the Enterprise Event Bus from planning your approach to completing technical set up. Working through these steps can seem daunting, but the Event Bus team is here to support you through this process. </p>"},{"location":"administrative-requirements/#product-planning","title":"Product Planning","text":"<p>Before diving into technical details, take some time to clarify your goals and approach: </p> <ul> <li>What problem are you solving? Document current pain points, inefficiencies, or limitations that event-driven architecture would address</li> <li>What is the desired outcome? Define specific improvements you expect to achieve</li> <li>How might you define and track success metrics? Identify measurable indicators like reduced latency, improved consistency, or enhanced user experience</li> <li>What is the timeline for this work? Consider the priority level and whether external deadlines, dependencies, or key milestones will influence when this needs to be completed.</li> <li>Have you identified data sources? While not required upfront, knowing which systems generate the events you need and who owns that data will streamline discussions and technical planning.</li> </ul>"},{"location":"administrative-requirements/#getting-connected","title":"Getting Connected","text":"<p>Once you\u2019ve thought through your use case, here\u2019s how to get the ball rolling:</p> <ul> <li>Set a meeting with Enterprise Event Bus to discuss use case and technical overview (You can find information on reaching out on the Contact and Support Page)</li> <li>Collaborate on a Working Agreement with the Enterprise Event Bus team and they will draft this based on your discussion </li> <li>Review and sign Enterprise Event Bus Working Agreement</li> </ul>"},{"location":"administrative-requirements/#system-registration","title":"System Registration","text":"<p>You\u2019ll need to gather system identification information. This helps the Event Bus team verify the system is authorized to connect and handle data appropriately:  </p> <ul> <li>Identify your VA Systems Inventory (VASI) ID</li> <li>Identify your Enterprise Mission Assurance Support Service (eMASS) ID</li> <li>As part of setting up your eMASS ID you should have already completed the System Categorization process to obtain a FISMA rating (Low, Moderate, or High)</li> <li>Identify your Authority to Operate (ATO) (or cATO if in LHDI)</li> <li>If building a web application, request Web Application Security Assessment (WASA) (requires 30-day notice)</li> </ul> <p>Note: If you do not already have an eMASS ID, VASI ID, and an ATO/cATO you will need to submit a LEAF intake request for the ones you are missing (LHDI tenants will need to consult with their LHDI Enablement Liaison)</p>"},{"location":"administrative-requirements/#privacy-documentation","title":"Privacy Documentation","text":"<p>These documents help ensure data protection compliance: </p> <ul> <li>Complete Privacy Threshold Analysis (PTA)</li> <li>Complete Privacy Impact Assessment (PIA)</li> </ul>"},{"location":"administrative-requirements/#special-requirements","title":"Special Requirements","text":"<p>Depending on your specific use case, you may also need to complete:</p> <ul> <li>OBI Data Access Form, which is required for BIP-sourced events </li> <li>Sensitivity Filtering implementation,  which is required for VA application user interfaces accessing the Corporate database</li> </ul>"},{"location":"administrative-requirements/#network-configuration","title":"Network Configuration","text":"<p>Finally, let\u2019s make sure your systems can talk with Event Bus:</p> <ul> <li>Test connection from your system to Enterprise Event Bus</li> <li>If you are blocked, work with the Event Bus team to submit an Enterprise Security External Change Council (ESECC) request<ul> <li>You will need to provide IP addresses (or CIDR blocks) and your system's connection ID assigned by VA NSOC</li> </ul> </li> </ul> <p>You can find more details on each of these topics below</p>"},{"location":"administrative-requirements/#authority-to-operate-ato","title":"Authority to Operate (ATO)","text":"<p>ATO, or \u201cAuthority to Operate,\u201d indicates that your system has been evaluated by VA and given permission to deploy code in production.</p> <p>Teams that have an active ATO should review it to understand the implications of integrating with other systems.</p>"},{"location":"administrative-requirements/#for-teams-without-an-ato","title":"For Teams without an ATO","text":"<p>For clients that are not LHDI tenants:</p> <ul> <li>ATO is evaluated once eMASS system registration has been completed and approved. Teams that do not have an active ATO should begin the process as early as possible in the Enterprise Event Bus integration process.</li> </ul> <p>For clients that are LHDI tenants:</p> <ul> <li>Teams within the Lighthouse Delivery Infrastructure should consult their LHDI Enablement Liaison to begin the ATO process.</li> </ul>"},{"location":"administrative-requirements/#wasa-testing","title":"WASA Testing","text":"<p>Web Application Security Assessment (WASA) scanning may be needed as part of the ATO process when a client system builds a web application for consuming events from the Event Bus. Please note that a 30 day notice is required for WASA testing.</p> <p>The VA SAVD WASA Coordination team can be contacted via email at <code>VASAVDWASACoordination[at]va.gov</code>. A request for WASA testing can be submitted on the Security Assessment Portal home page (must be on VA network to view).</p>"},{"location":"administrative-requirements/#vasi-and-emass","title":"VASI and eMASS","text":"<p>The client system should either have or be in the process of procuring: </p> <ul> <li>a VASI ID, which identifies your system in VA, and</li> <li>an eMASS ID</li> </ul> <p>Note: The VASI and eMASS Processes have merged and are now accomplished via a single LEAF intake (formerly GRC intake) request: Unified System Registry Intake (must be on VA network to view).</p>"},{"location":"administrative-requirements/#vasi","title":"VASI","text":"<p>The VA Systems Inventory (VASI) is intended to be a registry of all applications in use at VA. </p> <p>The Enterprise Event Bus VASI ID is 3325, and this is our entry (must be on VA network to view) in the VASI registry. You may find your system\u2019s VASI ID by visiting the VA System Inventory (must be on VA network to view).</p>"},{"location":"administrative-requirements/#for-teams-without-a-vasi-id","title":"For Teams without a VASI ID","text":"<p>For clients that are not LHDI tenants:</p> <ul> <li>Prospective client systems that do not have a VASI ID will need to submit a LEAF intake request to get one. </li> </ul> <p>For clients that are LHDI tenants:</p> <ul> <li>Teams on the Lighthouse Delivery Infrastructure (LHDI) should consult their LHDI Enablement Liaison to submit a LEAF intake request. </li> </ul>"},{"location":"administrative-requirements/#emass","title":"eMASS","text":"<p>Enterprise Mission Assurance Support Service (eMASS) is VA\u2019s Governance, Risk and Compliance (GRC) tool. All systems will be assessed in eMASS by the Risk Review team for an authorization recommendation to be submitted to the Authorizing Official (AO) for final ATO consideration.</p> <p>All VA systems are given an entry with a unique number in the eMASS system. The Enterprise Event Bus eMASS ID is 2350. </p> <p>Getting an eMASS ID is a prerequisite for System Categorization.</p>"},{"location":"administrative-requirements/#for-teams-without-an-emass-id","title":"For Teams without an eMASS ID","text":"<p>For clients that are not LHDI tenants:</p> <ul> <li>Prospective client systems that do not have an eMASS ID will need to submit a LEAF intake request to get one.</li> </ul> <p>For clients that are LHDI tenants:</p> <ul> <li>Teams on the Lighthouse Delivery Infrastructure (LHDI) should consult their LHDI Enablement Liaison to submit a LEAF intake request. </li> </ul>"},{"location":"administrative-requirements/#fisma-system-categorization","title":"FISMA System Categorization","text":"<p>As part of the eMASS process, System Categorization will evaluate the impact to the organization of loss or compromise to the data in the application. The outcome of the System Categorization process is a FISMA risk level rating of Low, Moderate, or High.</p> <p>If a prospective client system has not been through System Categorization, it will need to follow the steps outlined on this GRC System Categorization page (must be on VA network to view). Note that completing a Privacy Threshold Analysis (PTA) document is a prerequisite for System Categorization.</p> <p>Note: For teams handling ePHI (medical information specific to an individually identifiable patient), the HIPAA Security Rule applies and further review will be required. Support for this can be requested by sending an email to <code>VHAHCSDevelopmentSecurity2[at]va.gov</code>.</p>"},{"location":"administrative-requirements/#consumers-of-bip-sourced-events","title":"Consumers of BIP-sourced Events","text":""},{"location":"administrative-requirements/#obi-access-form","title":"OBI Access Form","text":"<p>Event Bus Consumers consuming a BIP-sourced event that is available on the Event Bus must follow the OBI Data Access Form process (aka the Corp DB Application Access Form).</p> <p>Who needs to submit an approval request?</p> <p>Anyone who needs to access Corporate database data and has not been previously approved for that specific purpose must submit a request.</p> <p>Examples:</p> <ul> <li>Any entity (Event Bus or an Event Bus Consumer) accessing Corporate data for the first time. </li> <li>Any entity (Event Bus or an Event Bus Consumer) who currently accesses specific Corporate data through an event on the Event Bus but now needs access to different data.</li> <li>Any entity who wants to use the Corporate data they have access to for a different purpose than what was previously approved; for example, An Event Bus Consumer has been using an event sourced from Corp DB data to trigger notifications and now wants to use that same event to automatically update data in another VA system.</li> <li>Any entity accessing Corporate data through a third party; for example, when a system consumes a BIP-sourced event from the Event Bus.</li> </ul> <p>Note: If both the Event Bus and the Consumer are consuming an event for the first time, the Consumer must wait for the Event Bus to submit and get approval for its own OBI Data Access Form request before submitting its own request.</p> <p>Instructions for filling out the OBI Data Access form (must be part of VA GitHub organization to view) and an example of the OBI Data Access form (must be part of VA GitHub organization to view) are available in the Enterprise Event Bus GitHub repository. </p> <p>While permission may be granted to access Corp DB data for use in both non-production and production environments, there are cases, such as when a team is in the process of obtaining its ATO, that access may be granted for use in lower environments first, with production access granted once the team's ATO has been registered in eMASS. In the case of the latter, it may be acceptable to submit only one request, as long as ATO confirmation is provided at some point during the Event Bus integration process.</p>"},{"location":"administrative-requirements/#sensitivity-filtering","title":"Sensitivity Filtering","text":"<p>The implementation of Sensitivity Filtering is required when event data, or data spawned from a callback initiated by the data, will be displayed through a VA application user interface to a VA employee or contractor.</p> <p>An article on sensitivity filtering (must be part of VA GitHub organization to view) and instructions for implementing Sensitivity Filtering (must be part of VA GitHub organization to view) can be found in the Enterprise Event Bus GitHub repository. </p>"},{"location":"administrative-requirements/#esecc","title":"ESECC","text":"<p>A request to the Enterprise Security External Change Council (ESECC) may be needed to authorize an opening in the firewall to enable a connection between the client system and the Enterprise Event Bus.</p> <p>Prior to determining if your system will require an ESECC, we will collaborate on testing a connection from your system to the Enterprise Event Bus. If an error message indicates that traffic is blocked at the network level, then an ESECC will be required.</p> <p>If an ESECC is required, the Enterprise Event Bus team will submit the ESECC request on your team's behalf. We will need the following information:</p> <ul> <li>The IP address of your system. This can be a list of specific IP addresses or CIDR blocks. If your system is deployed to a few IP addresses that are always the same, then we will use those specific IP addresses. If your system is deployed to different IP addresses within a CIDR block or a few CIDR blocks, then we'll use CIDR blocks.</li> <li>Your system's connection ID assigned by VA NSOC (VA Network Security Operations Center).</li> </ul>"},{"location":"administrative-requirements/#pta-and-pia","title":"PTA and PIA","text":"<p>A PTA, or Privacy Threshold Analysis, is a required document used to determine if a system is privacy-sensitive and requires additional privacy compliance documentation such as a PIA or SORN.\u202f It is also the first step of the privacy compliance documentation process.\u202f</p> <p>A PIA, or Privacy Impact Assessment, is a public document that describes:\u202f </p> <ul> <li>What PII the system is collecting\u202f </li> <li>Why the PII is being collected</li> <li>How the PII will be collected, used, accessed, shared, safeguarded, and stored\u202f</li> </ul> <p>Whenever there is a change to the data being shared between VA systems, such as a new Event Bus integration or change to an existing Event Bus integration, the Privacy Office should be consulted, regardless of whether it is a major change or not, in order to make a determination that an updated PTA or PIA may be required.</p> <p>PTAs are renewed annually and PIAs are renewed every three years, unless there are major changes. Submitting a new PTA restarts the clock on annual renewal.</p> <p>*Note that all producing and consuming client systems are subject to their own PTA and PIA reviews.</p> <p>Resources:</p> <ul> <li>Privacy Threshold Analysis (PTA) and Privacy Impact Assessment (PIA) Submittal Checklist and Process Overview (PDF file, must be on VA network to view)</li> <li>PTA Training Resources (VA SharePoint, must be on VA network to view)</li> <li>PIA Training Resources (VA SharePoint, must be on VA network to view)</li> </ul>"},{"location":"administrative-requirements/#other-resources","title":"Other Resources","text":"<p>For clients that are not LHDI tenants:</p> <ul> <li>OIS Official Documentation on the VASI/eMASS/ATO process: eMass Authorization Requirements SOP Guide (PDF file, must be on VA network to view)</li> </ul> <p>For clients that are LHDI tenants:</p> <ul> <li>Lighthouse Delivery Infrastructure Onboarding Process Overview (Mural document, view only)</li> </ul>"},{"location":"consume-events/","title":"Consuming Events","text":""},{"location":"consume-events/#whats-a-consumer","title":"What\u2019s a consumer?","text":"<p>A consumer is an application that is set up to receive messages or events in an event-driven system. The Event Bus exposes streams of events, called topics, to consumers. The events capture significant occurrences taking place in an external system. Find out how to view a list of currently available topics by visiting our Event Catalog page.</p> <p>To access messages in a particular topic, an event consumer subscribes to the topic and receive events as they occur in real time. This allows consumers to perform actions based on the event data, such as updating internal state, triggering other processes, etc. The content below outlines the steps needed to start consuming events. Learn more about the components and processes involved in event-based systems on our How Event Bus Implements Event-Driven Architecture page.</p>"},{"location":"consume-events/#steps-to-become-a-consumer","title":"Steps to become a consumer","text":""},{"location":"consume-events/#find-eventstopics-to-consume","title":"Find events/topics to consume","text":"<p>The first step to consuming an event is to reach out to the Enterprise Event Bus Team about your interest in events. From there, you can either subscribe to an existing topic with relevant events, or else identify a team able to provide the topic that is of interest to you. At this point in time, we are unable to identify producers for consumers that do not have a source for their desired events, but we will do our best to work with your chosen producing team.</p> <p>See also our Produce Events page.</p>"},{"location":"consume-events/#determine-if-you-need-an-esecc-request","title":"Determine if you need an ESECC request","text":"<p>See the ESECC section on the Administrative Requirements page.</p>"},{"location":"consume-events/#for-consumers-of-bip-sourced-events","title":"For consumers of BIP-sourced Events","text":"<p>Read the documentation about requesting data access and sensitivity filtering on our Administrative Requirements page.</p>"},{"location":"consume-events/#set-up-authorization-and-authentication","title":"Set up authorization and authentication","text":"<p>To subscribe to specific topics on the Event Bus, consumers need to be authenticated and have the appropriate permissions. The Event Bus MSK cluster is only accessible from the VA Network, and we use AWS IAM (Identity and Access Management) Roles and Policies to control access to different resources. If your consuming application is within the AWS environment, you will need to let us know to which IAM Role(s) or IAM User(s) we should grant access. We will then set up the corresponding IAM Policies on our end and assign a named role for consumers to authenticate with AWS MSK in their application code.</p> <p>If your consuming application is outside of the AWS environment, we will request an IAM User to be created on your behalf. You will then be able to access the requested topic(s) using those credentials.</p>"},{"location":"consume-events/#connect-to-the-event-bus-in-the-development-environment","title":"Connect to the Event Bus in the development environment","text":"<p>Once the authentication and authorization steps have been completed, you will receive the Kafka bootstrap server addresses and port numbers with which you can connect to the Event Bus MSK cluster. The following ports are open for consumers and producers that are authenticated with AWS IAM:</p> <ul> <li>9098 (for access from AWS)</li> <li>9198 (for access from outside of AWS)</li> </ul>"},{"location":"consume-events/#develop-and-deploy-your-consumer-application","title":"Develop and deploy your consumer application","text":"<p>Many programming languages and frameworks offer libraries designed to interact with Kafka. To ensure full compatibility with the Event Bus, your code needs to authenticate with the AWS MSK cluster using the assigned role provided during the onboarding process. Additionally, consumers should reference the Confluent Schema Registry and use the appropriate schema to deserialize messages in Avro.</p>"},{"location":"consume-events/#client-properties","title":"Client properties","text":"<p>To connect to the Event Bus, consumers in all programming languages will need to set these properties, which are required unless otherwise specified:</p> Property Value Description Notes bootstrap.servers List of one or more Event Bus brokers. This will vary depending on the environment (dev, prod, etc.). A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. Only one of the Event Bus brokers needs to be included in this list, but including more than one will ensure the application can start up if one of the Kafka servers is down. group.id This can be set to any value a consumer wants. A unique string that identifies the consumer group this consumer belongs to. sasl.mechanism <code>OAUTHBEARER</code> SASL mechanism used for client connections. security.protocol <code>SASL_SSL</code> Protocol used to communicate with brokers. auto.offset.reset (recommended) <code>earliest</code> What to do when there is no initial offset in Kafka or if the current offset does not exist anymore on the server. Setting this field to \"earliest\" instead of the default \"latest\" will cause the consumer to consume older events which are still available on the Event Bus when it first deploys. If past events do not need to be processed when first connecting to the Event Bus, then consumers can use the default. Once a consumer has consumed some events, it will always pick up where it last left off even when it restarts. client.rack (recommended) The AWS Availability Zone in which your application is running, eg. \"usgw1-az2\" A rack identifier for this client. This can be any string value which indicates where this client is physically located. This property is only applicable if your application is deployed to AWS (Amazon Web Services) infrastructure. Setting this to your availability zone will reduce network traffic costs. enable.auto.commit (recommended) false If true the consumer's offset will be periodically committed in the background. If set to true, the consumer may mark some records as consumed before they have been processed. See Manual Offset Control for more information. <p>Depending on the client language used, additional properties may also be needed for authorization and connecting to the schema registry. For example, these properties are required for Java clients:</p> Property Value Description Notes key.deserializer <code>KafkaAvroDeserializer</code> Deserializer class for key. All Event Bus records use an Avro schema, so this is required even if the key itself is a primitive type like <code>string</code> or <code>long</code>. sasl.jaas.config <code>OAuthBearerLoginModule</code> and role settings. The role will vary for each consumer. JAAS login context parameters for SASL connections in the format used by JAAS configuration files. See specifying an AWS IAM role for more information. sasl.login.callback.handler.class <code>IAMOAuthBearerLoginCallbackHandler</code> The fully qualified name of a SASL login callback handler class. See aws-msk-iam-auth for more information. sasl.client.callback.handler.class <code>IAMOAuthBearerLoginCallbackHandler</code> The fully qualified name of a SASL client callback handler class. See aws-msk-iam-auth for more information. value.deserializer <code>KafkaAvroDeserializer</code> Deserializer class for value. schema.registry.url Event Bus schema registry endpoint. This will vary depending on the environment (dev, prod, etc.). Comma-separated list of URLs for Schema Registry instances that can be used to register or look up schemas. use.latest.version <code>false</code> (this is the default value) Flag that indicates if the latest schema version should be used for deserialization. Event Bus recommends setting this value to false to avoid issues when a new schema version is added to the schema registry. latest.cache.ttl.sec <code>-1</code> (this is the default value) This sets a TTL for the schema registry cache. <code>-1</code> indicates that the cache has no TTL. Event Bus recommends using the default of <code>-1</code> for this value. Schema versions do not change once they are registered. This will decrease the application's dependency on the schema registry."},{"location":"consume-events/#code-samples","title":"Code samples","text":"<p>Info</p> <p>Expand the sections below to see consumer code examples in Java and Ruby. To see the consumer code samples in context, please check out the <code>ves-event-bus-sample-code</code> repository (must be part of VA GitHub organization to view).</p> Java Consumer <pre><code>package gov.va.eventbus.example;\n\nimport org.apache.avro.generic.GenericRecord;\nimport org.apache.kafka.clients.CommonClientConfigs;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.common.config.SaslConfigs;\nimport org.apache.kafka.common.config.SslConfigs;\nimport org.apache.kafka.common.errors.WakeupException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\nimport io.confluent.kafka.serializers.KafkaAvroDeserializer;\nimport io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;\n\nimport java.time.Duration;\nimport java.util.Collections;\nimport java.util.Properties;\n\npublic class TestConsumer implements Runnable {\n    private static final Logger LOG = LoggerFactory.getLogger(TestConsumer.class);\n    private final AtomicBoolean shutdown = new AtomicBoolean(false);\n\n    // Consumer values\n\n    // Set the topic you want to consume from\n    private static final String TOPIC = \"test\";\n    private static final String EB_BOOTSTRAP_SERVERS = System.getenv(\"EB_BOOTSTRAP_SERVERS\");\n    private static final String EB_SECURITY_PROTOCOL = System.getenv(\"EB_SECURITY_PROTOCOL\");\n    private static final String SCHEMA_REGISTRY_URL = System.getenv(\"SCHEMA_REGISTRY_URL\");\n    private static final String AWS_ROLE = System.getenv(\"AWS_ROLE\");\n\n    private final KafkaConsumer&lt;Long, User&gt; consumer;\n\n    public TestConsumer() {\n        this.consumer = createConsumer();\n    }\n\n    public void run() {\n        try {\n\n            consumer.subscribe(Collections.singletonList(TOPIC));\n\n            while (!shutdown.get()) {\n                ConsumerRecords&lt;Long, User&gt; records = consumer.poll(Duration.ofMillis(100));\n\n                for (ConsumerRecord&lt;Long, User&gt; record : records) {\n                    User user = record.value();\n                    // Process the received Avro record\n                    LOG.info(\"Received record: {}\", user.toString());\n                }\n            }\n        } catch (final WakeupException e) {\n            // Ignore exception if shutting down\n            if (!shutdown.get()) {\n                throw e;\n            }\n        } catch (final Exception e) {\n            LOG.error(\"An exception occurred while consuming messages\", e);\n        } finally {\n            consumer.close();\n        }\n    }\n\n    /**\n    * Stops polling for new messages and wakes up the Kafka consumer.\n    */\n    public void shutdown() {\n        shutdown.set(true);\n        consumer.wakeup();\n    }\n\n    private KafkaConsumer&lt;Long, User&gt; createConsumer() {\n        final Properties props = new Properties();\n\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, EB_BOOTSTRAP_SERVERS);\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());\n        props.put(KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG, SCHEMA_REGISTRY_URL);\n        // ensure records with a schema are converted.\n        props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, \"test-consumer-group\"); // Set your consumer group ID\n\n        // Use SASL_SSL in production but PLAINTEXT in local environment\n        // w/docker_compose\n        if (\"SASL_SSL\".equals(EB_SECURITY_PROTOCOL)) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, EB_SECURITY_PROTOCOL);\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"/tmp/kafka.client.truststore.jks\");\n            props.put(SaslConfigs.SASL_MECHANISM, \"OAUTHBEARER\");\n            props.put(SaslConfigs.SASL_JAAS_CONFIG,\n                    \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required awsRoleArn=\\\"\"\n                            + AWS_ROLE // use the role name provided to you\n                            + \"\\\" awsStsRegion=\\\"us-gov-west-1\\\";\");\n            props.put(SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS,\n                    \"software.amazon.msk.auth.iam.IAMOAuthBearerLoginCallbackHandler\");\n            props.put(SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS,\n                    \"software.amazon.msk.auth.iam.IAMOAuthBearerLoginCallbackHandler\");\n        } else if (!\"PLAINTEXT\".equals(EB_SECURITY_PROTOCOL)) {\n            LOG.error(\"Unknown EB_SECURITY_PROTOCOL '{}'\", EB_SECURITY_PROTOCOL);\n        }\n\n        return new KafkaConsumer&lt;&gt;(props);\n    }\n}\n</code></pre> Ruby Consumer <pre><code>require 'logger'\nrequire 'rdkafka'\nrequire 'avro_turf/messaging'\nrequire_relative 'oauth_token_refresher'\n\nlogger = Logger.new(STDOUT)\n\n@consumers = {}\n\ndef refresh_token(_config, consumer_name)\n    consumer = @consumers[consumer_name]\n    OAuthTokenRefresher.new.refresh_token(consumer)\nend\n\nsecurity_protocol = ENV['SECURITY_PROTOCOL']\n\nproperties = {\n    'bootstrap.servers': ENV['KAFKA_HOST'],\n    'group.id': 'sample-ruby-consumer',\n    'security.protocol': security_protocol,\n    'enable.auto.commit': false,\n    'auto.offset.reset': 'earliest'\n}\n\nif 'SASL_SSL' == security_protocol.upcase\n    properties['sasl.mechanisms'] = 'OAUTHBEARER'\n    Rdkafka::Config.oauthbearer_token_refresh_callback = method(:refresh_token)\nend\n\nconsumer = Rdkafka::Config.new(properties).consumer(native_kafka_auto_start: false)\n@consumers[consumer.name] = consumer\nconsumer.start\nconsumer.subscribe(\"appointments\")\n\navro = AvroTurf::Messaging.new(registry_url: ENV['SCHEMA_REGISTRY_URL'], registry_path_prefix: ENV['SCHEMA_REGISTRY_PATH_PREFIX'])\n\nlogger.info \"Running consumer\"\nconsumer.each do |message|\n    logger.info \"Message received: #{message}\"\n    logger.info \"Decoded message payload: #{avro.decode(message.payload)}\"\n    consumer.commit\nend\n</code></pre>"},{"location":"consume-events/#register-with-code-va","title":"Register with CODE VA","text":"<p>CODE VA (must be on VA network to view) is a software catalog that houses information about software entities from across VA. Once your consumer application is up and running, it's important to register with the catalog so event producers are aware of how their events are being used and which systems are consuming them.</p> <p>To register with CODE VA:</p> <ol> <li>In CODE VA, an event-consuming software entity can be modeled as a Component or as a System. If you are unsure whether to classify your consumer as a Component or a System, see the Backstage System Model.</li> <li>Create a file named <code>catalog-info.yaml</code> at the root of your source code repository and populate it with the applicable template, updating <code>metadata</code> and <code>spec</code> with values that correspond to your component or system.</li> <li>Once your <code>catalog-info.yaml</code> file has been committed it will be automatically processed and the software entity will be viewable on CODE VA website (must be on the VA network to view) within a few hours. If you would like the software entity to display quicker, follow the Backstage documentation on the default method for adding entries to the catalog.</li> </ol>"},{"location":"consume-events/#component-template","title":"Component Template","text":"<pre><code>    apiVersion: backstage.io/v1alpha1\n    kind: Component\n    metadata:\n        name: component-name\n        description: Component description.\n        title: Component Name\n        links:\n            - url: https://sample-slack-link.com\n            title: Event Consumer Slack Channel\n            - url: https://sample-link.com\n            title: Component Documentation\n    spec:\n        type: service\n        lifecycle: production\n        owner: owning-team\n        subscribesToEvent: [event-name, event-name-two]\n</code></pre> <p>Here is some additional information on these fields:</p> <ul> <li>apiVersion [required]: This value must be set to <code>backstage.io/v1alpha1</code>.</li> <li>kind [required]:  This value must be set to <code>Component</code>.</li> <li>metadata [required]: A structure that contains information about the entity. The <code>metadata</code> structure includes the following properties.<ul> <li>name [required]: A machine-readable name for the component. This value will be used in CODE VA urls, so it should be all lowercase and use hyphens as separators.</li> <li>description [required]: A concise, high-level description of the event-consuming component.</li> <li>title [required]: A human-readable representation of the <code>name</code> to be used in CODE VA user interfaces.</li> <li>links [optional]: A list of links related to the component. Each link consists of a <code>url</code> and a <code>title</code>.<ul> <li>url [required]: The external url that will be opened when the link is clicked.</li> <li>title [required]: Display text for the link.</li> </ul> </li> </ul> </li> <li>spec [required]: A structure that contains information about the component. The <code>spec</code> structure includes the following properties.<ul> <li>type [required]: The component type. Possible values include: <code>website</code>, <code>service</code>, <code>library</code>, etc.</li> <li>lifecycle [required]: The current development status for the component. Possible values include: <code>experimental</code>, <code>production</code>, <code>deprecated</code>, etc.</li> <li>owner [required]: The team that owns the event-consuming component. If this is set to the name of a GitHub team within the VA's GitHub organization, this field will link to a page with details about the team in CODE VA.</li> <li>subscribesToEvent [required]: An array of strings. Each string must match the <code>metadata.name</code> value of a producer's <code>catalog-info.yaml</code> file. This field is used to relate the component to the events that it consumes and to display the component on each related event's CODE VA catalog entry.</li> </ul> </li> </ul> <p>See Backstage's Component documentation for more information about additional optional fields.</p>"},{"location":"consume-events/#system-template","title":"System Template","text":"<pre><code>    apiVersion: backstage.io/v1alpha1\n    kind: System\n    metadata:\n        name: system-name\n        description: System description.\n        title: System Name\n        links:\n            - url: https://sample-slack-link.com\n            title: Event Consumer Slack Channel\n            - url: https://sample-link.com\n            title: System Documentation\n    spec:\n        owner: owning-team\n        domain: health\n        subscribesToEvent: [event-name, event-name-two]\n</code></pre> <p>Here is some additional information on these fields:</p> <ul> <li>apiVersion [required]: This value must be set to <code>backstage.io/v1alpha1</code>.</li> <li>kind [required]:  This value must be set to <code>System</code>.</li> <li>metadata [required]: A structure that contains information about the entity. The <code>metadata</code> structure includes the following properties.<ul> <li>name [required]: A machine-readable name for the system. This value will be used in CODE VA urls, so it should be all lowercase and use hyphens as separators.</li> <li>description [required]: A concise, high-level description of the event-consuming system.</li> <li>title [required]: A human-readable representation of the <code>name</code> to be used in CODE VA user interfaces.</li> <li>links [optional]: A list of links related to the system. Each link consists of a <code>url</code> and a <code>title</code>.<ul> <li>url [required]: The external url that will be opened when the link is clicked.</li> <li>title [required]: Display text for the link.</li> </ul> </li> </ul> </li> <li>spec [required]: A structure that contains information about the system. The <code>spec</code> structure includes the following properties.<ul> <li>owner [required]: The team that owns the event-consuming system. If this is set to the name of a GitHub team within the VA's GitHub organization, this field will link to a page with details about the team in CODE VA.</li> <li>domain [optional]: The VA domain in which a particular system exists. Possible values might be: <code>claims status</code>, <code>health</code>, <code>appointments</code>, <code>benefits</code>, etc.</li> <li>subscribesToEvent [required]: An array of strings. Each string must match the <code>metadata.name</code> value of a producer's <code>catalog-info.yaml</code> file. This field is used to relate the system to the events that it consumes and to display the system on each related event's CODE VA catalog entry.</li> </ul> </li> </ul>"},{"location":"consume-events/#schema-evolution","title":"Schema Evolution","text":"<p>Eventually the schema of an event may evolve. After a new version of the schema is added to the schema registry, consumers will need to update their applications to handle the new schema version before producers update to produce events using the new schema. The Event Bus team will inform consumers about upcoming schema changes.</p>"},{"location":"consume-events/#logs","title":"Logs","text":"<p>Logs are stored within a LightHouse Delivery Infrastructure (LHDI) AWS S3 bucket. Only LHDI admins with AWS access can access this bucket and its content. Although producers and consumers will not have access to the S3 bucket directly, logs are available via Datadog (must have VA LightHouseDI Datadog access to view): - Event Bus broker logs sandbox - Event Bus broker logs Prod - Event Bus app logs sandbox - Event Bus app logs prod</p> <p>Datadog is a monitoring and analytics tool that is used within VA and is hosted by the Devops Transformation Services (DOTS) team. LHDI team members are admins within the Datadog space where the Event Bus metrics and logs are available. In order for Event Bus users to request access to Datadog they must have a VA email address. To request access to Datadog, complete the HelpDesk form on the ServiceNow Portal at ECC (Enterprise Command Center) Monitoring Services - your IT Service Portal (must be on the VA network to view).</p>"},{"location":"consume-events/#troubleshooting","title":"Troubleshooting","text":"<p>If you have questions or run into difficulties with any of these steps, please see our Contact and Support page.</p>"},{"location":"consumer-onboarding/","title":"Consumer Onboarding","text":"<p>Consumer onboarding documentation is coming soon! If you need to reach out to us right now, feel free to file a github issue here</p>"},{"location":"environments/","title":"Environments","text":"<p>The Enterprise Event Bus' Apache Kafka instances and associated applications are deployed into several environments for different purposes. We deploy any new releases to all environments on Wednesdays. The environments are described in the table below.</p> Environment Name Description PHI, PIIAllowed <code>sandbox</code> A stable environment that external teams can connect to. It is a \"customer dev\" environment. No <code>prod</code> Our production environment. Yes"},{"location":"get-support/","title":"Event Bus Contact and Support","text":""},{"location":"get-support/#learn-more-about-enterprise-event-bus","title":"Learn More About Enterprise Event Bus","text":"<p>Interested in learning how Enterprise Event Bus (EEB) can benefit you? Our team is ready to help you understand the capabilities, architecture, and best practices for EEB implementation.</p>"},{"location":"get-support/#quick-reference","title":"Quick Reference","text":"Support Type Contact Method Response Time General Contact OCTO Slack: #ves-event-bus 1 hour (during support hours) General Contact Email: OITEnterpriseEventBus@va.gov Varies Urgent/Critical Incidents and Security/Privacy Incidents OCTO Slack: #event-bus-support 1 hour (during support hours) Urgent/Critical Incidents and Security/Privacy Incidents Email: found on OCTO Slack #event-bus-support 30 minutes"},{"location":"get-support/#general-contact-and-support","title":"General Contact and Support","text":"<p>To learn more or for non-urgent questions:</p> <ul> <li>OCTO Slack: #ves-event-bus in the Office of CTO @VA workspace</li> <li>Email: OITEnterpriseEventBus@va.gov</li> </ul> <p>Best Practice: Please use the public channel rather than direct messaging team members. This helps others learn from shared questions and answers.</p>"},{"location":"get-support/#support-hours","title":"Support Hours","text":"<ul> <li>Slack Support Hours: Monday - Friday, 12pm - 5pm ET (excluding federal holidays)</li> <li>Note: Engineers often respond by 9am ET if they're in Eastern or Central timezones</li> <li>24/7 Support: Available for urgent incidents via PagerDuty email which can be found on OCTO Slack #event-bus-support  (Must email from a @va.gov address. If you do not have a @va.gov email address, see below.)</li> </ul>"},{"location":"get-support/#reporting-urgent-incidents","title":"Reporting Urgent Incidents","text":"<p>When you need immediate assistance with a critical operational, security, or privacy incident:</p> <ol> <li>Check the #event-bus-support OCTO Slack channel first</li> <li>Email the PagerDuty email from a @va.gov email address</li> <li>This will alert the on-call engineer's mobile device 24/7</li> <li>Response expected within 30 minutes in the #event-bus-support OCTO Slack Channel</li> </ol> <p>Include in your email:</p> <ul> <li>Is this a security or privacy incident? (Yes/No)</li> <li>Which environment is impacted? (Sandbox/Production)</li> <li>Brief description of the incident's impact</li> </ul>"},{"location":"get-support/#using-non-va-email-addresses","title":"Using Non-VA Email Addresses","text":"<p>If you need to use a non @va.gov email address to contact the on-call engineer:</p> <ol> <li>Send a message to the #event-bus-support OCTO Slack channel</li> <li>Request your email address or domain be added to the \"allowed list\"</li> <li>Explain your specific need for using a non-VA email address</li> </ol>"},{"location":"get-support/#monitoring-and-alerts","title":"Monitoring and Alerts","text":"<p>Stay informed about Event Bus status through these Slack channels:</p> <ul> <li>Sandbox Environment: #ves-event-bus-sandbox-alerts (must be in the Office of CTO @VA workspace)</li> <li>Production Environment: #ves-event-bus-prod-alerts (must be in the Office of CTO @VA workspace)</li> <li>LHDI Environment: #vaapi-lhdi-customer-env-alerts (must be in the Lighthouse workspace)</li> </ul>"},{"location":"intro-to-eda/","title":"How Event Bus Implements Event-driven Architecture","text":""},{"location":"intro-to-eda/#introduction","title":"Introduction","text":"<p>The Enterprise Event Bus is an asynchronous event processing system that spans systems and lines of business at VA. Event-driven architecture uses events \u2014 types of actions, such as a Veteran creating a medical appointment or updating their beneficiaries \u2014 to communicate with systems that are subscribed to the stream of events. The systems that are producing the events, also known as Producers, are decoupled from the systems that are consuming their events, also known as Consumers.</p>"},{"location":"intro-to-eda/#key-benefits","title":"Key Benefits","text":"<ul> <li>Improved responsiveness and scalability</li> <li>Real-time data processing</li> <li>Reduced system coupling</li> <li>Enhanced system reliability</li> </ul>"},{"location":"intro-to-eda/#how-events-work","title":"How Events Work","text":"<p>An event happens when:</p> <ul> <li>A person or automated process takes an action and changes the state of the system in which it occurs, usually by creating new or updating existing data</li> <li>That action prompts the Producer system to publish the new or revised information to a streaming queue </li> <li>Subscribers known as Consumers receive the updated information</li> <li>Those Consumers then share that information to take action (e.g., sharing information with Veterans or with another VA system)</li> </ul> <p>The following conceptual diagram illustrates how Producers and Consumers might interact with the Enterprise Event Bus. Producers publish many different kinds of events. Consumers may do many different things with event data, such as notify a Veteran or kick off a workflow. View the full-sized diagram (must be part of VA GitHub organization to view).</p> <p></p>"},{"location":"intro-to-eda/#role-within-the-va-enterprise-environment","title":"Role within the VA enterprise environment","text":"<p>We define enterprise events as those that capture significant occurrences within VA, such as when a Veteran\u2019s benefit claim reaches certain meaningful milestones.</p> <p>We strive to enable connecting previously disparate systems that live across different business lines within VA, such as the Veteran Benefit Administration (VBA) and the Veteran Health Administration (VHA). As such, we are not limited to a specific business line or domain within VA.</p> <p>It should be noted that Enterprise Event Bus does not aspire to be the only event streaming platform within VA, but one that is available to any team that wants to produce or consume enterprise events. This would allow teams who don\u2019t have the time or resources to set up their own event streaming platform to take advantage of event-driven technologies. It is not intended to subsume or replace any existing VA API ecosystems, nor is it a one-stop shop or mandatory runtime environment for all VA data services; instead, it is intended to enable event-based architecture for current and future integrations between systems.</p>"},{"location":"intro-to-eda/#an-opinionated-conduit","title":"An opinionated conduit","text":"<p>We often refer to the Enterprise Event Bus team as an opinionated conduit between event Consumers and Producers across the VA ecosystem. That\u2019s because our research, hands-on experience, and deep knowledge of Apache Kafka best practices have led us to definite opinions regarding:</p> <ul> <li>The type of events that best fit an Enterprise Event Bus architecture (more on that below)</li> <li>The choice of technologies with which we implement the Enterprise Event Bus across different contexts of use </li> <li>The most suitable infrastructure for implementing it. </li> </ul> <p>These enable us to provide Producers and Consumers with clear guidance to and assist with onboarding and learning about the underlying technologies.</p>"},{"location":"intro-to-eda/#enterprise-events-versus-data-events","title":"Enterprise events versus data events","text":"<p>The focus of the Enterprise Event Bus is enterprise events rather than data events. As described above, an event is a specific action that involves a change in data. An enterprise event is a business event that is of potential interest to a broad scope of consuming systems. </p> <p>For example, when Veterans apply for benefits, they are issued a benefits decision letter detailing VA\u2019s decision about the benefits for which they are eligible. The benefits decision letter becoming available is an enterprise event. The specific changes in the underlying data that indicate this document has become available are data events. Teams producing events onto the Enterprise Event Bus are encouraged to frame their events as enterprise events; that is, broadly speaking, an event that describes a concrete business event has the potential to be more useful to a broader audience than specific data change events. </p> <p>When determining what information an event should contain, a guiding principle is that Consumers need events to at least contain enough information to know whether the event is of interest to them, while minimizing the amount of sensitive data in the event payload. In general it is easier to add new pieces of information to a schema than it is to later remove it; thus the system should trend towards leaner events.</p>"},{"location":"intro-to-eda/#schemas-and-platform-independence","title":"Schemas and platform independence","text":"<p>Event Bus transports events as platform-independent byte streams (i.e., data broken down into its simplest form, sequences of bytes). This allows systems that store objects in different formats to communicate with each other without needing to understand each other's formats. This is done through the process of serialization/deserialization and the use of an event schema. Event schemas are stored in a schema registry.</p> <p>When a Producer application has event data to push to Event Bus, the event data will be serialized (converted from objects into byte streams), using rules defined in the version of the schema the Producer is using. Serializers allow Producers three options for setting the schema version: always use the latest schema version, pin to a specific schema version, or let the serializer dynamically infer the version from the event payload itself. We recommend pinning to a specific schema version to reduce ambiguity and to avoid issues that can arise from using the latest version if a new schema version is added to the schema registry.</p> <p>When a Consumer application pulls event data from Event Bus, the event data will be deserialized (converted from byte streams into objects), using rules defined in the version of the schema the Consumer is using. Deserializers allow Consumers two options for setting the schema version: always use the latest schema version, or use the schema version included in the event payload. We recommend using the schema version included in the event payload to avoid issues that can arise from using the latest version if a new schema version is added to the schema registry.</p> <p>Occasionally there may be schema updates and new schema versions will be added to the schema registry. This will require Producers and Consumers to update their applications. When an update is needed, we will direct consumer applications to update first, to maintain optimum compatibility across applications utilizing Event Bus.</p>"},{"location":"intro-to-eda/#technology-overview","title":"Technology overview","text":"<p>Enterprise Event Bus, like most other event-based systems in VA, is based on Apache Kafka, an open-source distributed event streaming platform. It is considered the industry standard for handling real-time data feeds, and is capable of handling the kinds of large scale, high-throughput loads we would expect an enterprise-wide event bus in VA to be able to handle. In addition to being a proven technology, Kafka is already used in a production capacity by other teams in VA, such as the Benefits Integration Platform.</p> <p>The Enterprise Event Bus uses AWS Managed Streams for Kafka (MSK), a hosted version of Kafka that runs in the VA Enterprise Cloud (VAEC). Producer systems publish events to Kafka topics that are available on AWS MSK. Events in a single topic are distributed across multiple brokers and partitions in order to balance load. Metadata managed by the cluster informs producing and consuming systems which broker they need to connect to. The data on AWS MSK is stored in AWS Elastic Block Store (EBS). Consumer systems can subscribe to topics to pull streams of events. The diagram below shows how events are stored in AWS MSK and how events are distributed from Producers to Consumers via AWS MSK.</p> <p></p>"},{"location":"intro-to-eda/#learn-more","title":"Learn more","text":"<ul> <li>Find definitions for acronyms and event-related terms on the Terminology page.</li> <li>Watch this brief video on YouTube for a quick overview of event-driven architecture.</li> <li>Learn about event streaming patterns on the Confluent Developer website.</li> <li>Watch a presentation on the Confluent Developer website about \u201cLessons Learned and Plans for the Future\u201d by Robert Ezekiel from Booz Allen Hamilton.</li> <li>Watch a presentation on the Confluent Developer website about \u201cImproving Veteran Benefit Services Through Efficient Data Streaming\u201d by Robert Ezekiel from Booz Allen Hamilton</li> <li>The Enterprise Event Bus Team offers consultations and is happy to answer questions! Please see our Contact and Support page.</li> </ul>"},{"location":"offboard/","title":"Offboarding","text":"<p>If your team intends to offboard from the Enterprise Event Bus, please begin by informing the Event Bus team as early as possible. We aim to have at least six months\u2019 notice so that we can coordinate with all impacted parties, including the producers or consumers of your deprecated event. </p> <p>Please see our Contact and Support page if you'd like to offboard from a specific event, or from the Event Bus altogether.</p>"},{"location":"produce-events/","title":"Producing Events","text":""},{"location":"produce-events/#whats-a-producer","title":"What's a producer?","text":"<p>A producer is an application designed to generate messages or events within an event-driven system. In the context of the Enterprise Event Bus, which aims to expose streams of events known as topics, producers play a crucial role. Producing teams have access to, or knowledge about, important data changes in the VA ecosystem and can send events to specific topics on the Event Bus. Find out how to view a list of currently available topics by visiting our Event Catalog page.</p> <p>As producers are responsible for defining topics and their contents, they will work with the Event Bus Team to determine configuration settings such as the number of topic partitions, event versioning, and event retention rules. The content below outlines the steps needed to start producing events. To learn more about the components and processes involved in event-based systems, please visit our How Event Bus Implements Event-Driven Architecture page.</p>"},{"location":"produce-events/#steps-to-become-a-producer","title":"Steps to become a producer","text":""},{"location":"produce-events/#define-the-event-or-topic","title":"Define the event or topic","text":"<p>The first step in producing an event is to contact the Enterprise Event Bus Team and express your interest in contributing an event stream. An ideal event stream represents a business event within the VA that would be of interest to multiple stakeholders and have a significant impact on Veterans. Examples of such events include changes to eligibility for benefits, milestones in the claims process, or updates to a Veteran's health record, such as new appointments, prescriptions, or lab results.</p> <p>While it is not a requirement to have consumers identified from the start, in an ideal scenario the events would theoretically be meaningful to multiple, independent consumers. If you do have consumers in mind, reach out to them and engage in preliminary discussions about their needs, preferences, and timelines. You can find more information on consuming events on our Consuming Events page.</p> <p>If there is a mutual interest to continue after the initial meeting, the Event Bus Team will create a one-page description of the proposed event topic, including details about the business context, event purpose, payload, and consumers. The partner team will review and provide feedback on the document and sign a Working Agreement and a Data Sharing Agreement.</p>"},{"location":"produce-events/#determine-if-you-need-an-esecc-request","title":"Determine if you need an ESECC request","text":"<p>See the ESECC section on the Administrative Requirements page.</p>"},{"location":"produce-events/#choose-configuration-settings-and-submit-onboarding-request","title":"Choose configuration settings and submit onboarding request","text":"<p>Before creating a topic, you need to consider various Kafka settings. Producing teams must choose appropriate settings based on their specific use case and technical requirements. From a logistical perspective, the Event Bus Team will handle the initial creation of the topic using the Kafka CLI and make it available for production.</p>"},{"location":"produce-events/#number-of-partitions","title":"Number of partitions","text":"<p>Partitions in Kafka serve as the primary unit of storage within a topic, with each partition containing a subset of events. Determining the number of partitions is crucial, as it has implications for storage, scalability, replication, and message movement. To learn more about reasonable defaults, and other partition-related concerns, read our guidance on topics and partitions (must be part of VA GitHub organization to view).</p>"},{"location":"produce-events/#topic","title":"Topic","text":"<p>We recommend all producer lower environments produce events the same topic in the Event Bus sandbox environment, though if you feel like you need multiple topics, please talk to us.</p>"},{"location":"produce-events/#event-retention","title":"Event retention","text":"<p>Event retention refers to how long an event exists within Kafka and remains available for consumption. This setting would be especially important to consumers who need to be prepared to handle missed events before they expire. For additional information and discussion, see the section about Retention in our Event Design Architectural Decision Record (must be part of VA GitHub organization to view).</p>"},{"location":"produce-events/#event-schema-and-event-registry","title":"Event schema and event registry","text":"<p>The Event Bus utilizes the Confluent Schema Registry to store schemas and their versions. Avro is used to define the schema format. Producers must submit an event schema representing the event payload in Avro format as part of the onboarding process. Producers must also use Apache Avro to serialize data onto the Event Bus so that the event schema, and the data contract it represents, is enforced. Additionally, producers should consider event versioning, which involves planning how schemas will evolve. Event versions are governed by the compatibility type setting, which determines allowed schema changes and how consumers interact with different versions. For most producers, we recommend using the <code>BACKWARD</code> compatibility type. For more information, see our article about the Confluent Schema Registry (must be part of VA GitHub organization to view), and our article about schema versioning (must be part of VA GitHub organization to view).</p> <p>Once the producing team has communicated their decisions about the event schema, the Event Bus team will create the topic, along with the schema, and notify the producing team when everything is ready for use.</p>"},{"location":"produce-events/#set-up-authorization-and-authentication","title":"Set up authorization and authentication","text":"<p>To produce messages to specific topics on the Event Bus, producers need to be authenticated and have the appropriate permissions. The Event Bus MSK cluster is only accessible from the VA Network, and we use AWS IAM (Identity and Access Management) Roles and Policies to control access to different resources. If your producing application is within the AWS environment, please inform us of the IAM Role(s) or IAM User(s) to which we should grant access. We will then set up the corresponding IAM Policies on our end and assign a named role for producers to authenticate with AWS MSK in their application code. If your producing application is outside of the AWS environment, we will request an IAM User to be created on your behalf. You will then be able to access the requested topic(s) using those credentials.</p>"},{"location":"produce-events/#connect-to-the-event-bus-in-the-development-environment","title":"Connect to the Event Bus in the development environment","text":"<p>Once the authentication and authorization steps have been completed, you will be able to connect to the Event Bus MSK cluster using the Kafka bootstrap server addresses and port numbers available in the Event Catalog. The following ports are open for consumers and producers that are authenticated with AWS IAM:</p> <ul> <li>9098 (for access from AWS)</li> <li>9198 (for access from outside of AWS)</li> </ul>"},{"location":"produce-events/#develop-and-deploy-your-producer-application","title":"Develop and deploy your producer application","text":"<p>Many programming languages and frameworks offer libraries designed to interact with Kafka. To ensure full compatibility with the Event Bus, your code needs to authenticate with the AWS MSK cluster using the assigned role provided during the onboarding process. Additionally, producers should reference the Confluent Schema Registry and use the created schema to serialize messages in Avro.</p>"},{"location":"produce-events/#client-properties","title":"Client properties","text":"<p>To connect to the Event Bus, producers in all programming languages will need to set these properties:</p> Property Value Description Notes bootstrap.servers List of one or more Event Bus brokers. This will vary depending on the environment (dev, prod, etc.). A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. Only one of the Event Bus brokers needs to be included in this list, but including more than one will ensure the application can start up if one of the brokers is down. enable.idempotence false When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. If 'false', producer retries due to broker failures, etc., may write duplicates of the retried message in the stream. If idempotence is a must-have for your application, then we will need to grant additional AWS permissions before you can set this to true. sasl.mechanism <code>OAUTHBEARER</code> SASL mechanism used for client connections. security.protocol <code>SASL_SSL</code> Protocol used to communicate with brokers. <p>Depending on the language client used, additional properties may also be needed for authorization and connecting to the schema registry. For example, these properties are required for Java clients:</p> Property Value Description Notes key.serializer <code>KafkaAvroSerializer</code> Serializer class for key. All Event Bus records use an Avro schema, so this is required even if the key itself is a primitive type like <code>string</code> or <code>long</code>. sasl.jaas.config <code>OAuthBearerLoginModule</code> and role settings. The role will vary for each producer. JAAS login context parameters for SASL connections in the format used by JAAS configuration files. See specifying an AWS IAM role for more information. sasl.login.callback.handler.class <code>IAMOAuthBearerLoginCallbackHandler</code> The fully qualified name of a SASL login callback handler class. See aws-msk-iam-auth for more information. sasl.client.callback.handler.class <code>IAMOAuthBearerLoginCallbackHandler</code> The fully qualified name of a SASL client callback handler class. See aws-msk-iam-auth for more information. value.serializer <code>KafkaAvroSerializer</code> Serializer class for value. auto.register.schemas <code>false</code> Specify if the serializer should attempt to register the schema with the Schema Registry. If set to true, the producer will attempt to register a new schema rather than using an existing one in the registry. Since writes to the Event Bus schema registry are blocked for unauthorized applications, this will result in an error which prevents the producer from producing events. schema.registry.url Event Bus schema registry endpoint. This will vary depending on the environment (dev, prod, etc.). Comma-separated list of URLs for Schema Registry instances that can be used to register or look up schemas. use.latest.version <code>false</code> (this is the default value) Flag that indicates if the latest schema version should be used for serialization. Event Bus recommends setting this value to false to avoid issues when a new schema version is added to the schema registry. use.schema.id ID of the schema in the Event Bus schema registry. This will vary depending on the environment. The Event Bus team will supply this value. Integer ID that indicates which schema to use for serialization. Event Bus recommends setting this value to be specific about which schema version is used to write events and to reduce ambiguity. latest.cache.ttl.sec <code>-1</code> (this is the default value) This sets a TTL for the schema registry cache. <code>-1</code> indicates that the cache has no TTL. Event Bus recommends using the default of <code>-1</code> for this value. Schema versions do not change once they are registered. This will decrease the application's dependency on the schema registry."},{"location":"produce-events/#producer-performance-optimization","title":"Producer Performance Optimization","text":"<p>Kafka producers can be optimized based on message size and throughput requirements. The default Kafka settings (<code>batch.size=16384</code> bytes, <code>linger.ms=0</code>) are designed for larger messages but can result in poor batch utilization for smaller messages.</p>"},{"location":"produce-events/#environment-variable-configuration","title":"Environment Variable Configuration","text":"<p>Configure via environment variables to allow per-environment optimization:</p> <pre><code>// In your dependency provider or configuration class\npublic final int KAFKA_PRODUCER_BATCH_SIZE;\npublic final int KAFKA_PRODUCER_LINGER_MS;\n\npublic DependencyProvider() {\n    // Kafka Producer optimization settings with defaults optimized for small messages\n    // Default Kafka settings: batch.size=16384 (16KB), linger.ms=0\n    // For small messages (104-204 bytes), this results in ~1.2% batch utilization (99% waste)\n    // \n    // Recommended settings based on message size:\n    // - Small messages (&lt;500 bytes): batch.size=1024 (1KB), linger.ms=5-10ms \u2192 ~20% utilization\n    // - Medium messages (500B-5KB): batch.size=8192 (8KB), linger.ms=5-10ms \u2192 ~20-60% utilization\n    // - Large messages (&gt;5KB): batch.size=16384 (16KB), linger.ms=0-5ms \u2192 ~30-100% utilization\n    //\n    // Trade-off: Adding linger.ms slightly increases latency but significantly improves throughput\n    // Example: 10ms linger allows batching 5-10 small messages, reducing network calls by 5-10x\n    KAFKA_PRODUCER_BATCH_SIZE =\n        Integer.parseInt(ENV.getOrDefault(\"KAFKA_PRODUCER_BATCH_SIZE\", \"16384\"));\n    KAFKA_PRODUCER_LINGER_MS = \n        Integer.parseInt(ENV.getOrDefault(\"KAFKA_PRODUCER_LINGER_MS\", \"0\"));\n}\n\nprivate Properties createProducerConfig() {\n    Properties producerProps = new Properties();\n\n    // ... other configuration ...\n\n    // Kafka producer performance tuning parameters\n    // Configured via environment variables to allow per-environment optimization\n    // Monitor impact using DataDog JMX metrics: batch-size-avg, record-send-rate, request-latency-avg\n    producerProps.put(ProducerConfig.BATCH_SIZE_CONFIG, KAFKA_PRODUCER_BATCH_SIZE);\n    producerProps.put(ProducerConfig.LINGER_MS_CONFIG, KAFKA_PRODUCER_LINGER_MS);\n\n    return producerProps;\n}\n</code></pre>"},{"location":"produce-events/#kubernetes-deployment-configuration","title":"Kubernetes Deployment Configuration","text":"<p>In your <code>deployment.yaml</code>:</p> <pre><code>env:\n  - name: KAFKA_PRODUCER_BATCH_SIZE\n    # Producer batch size optimization for small messages\n    # Default: 16384 (16KB) - results in ~1.2% utilization for 200-byte messages\n    # Current: {{ .Values.producer.batchSize }} - optimized for our message sizes\n    # Formula: Utilization = Avg Message Size \u00f7 batch.size\n    # See producer config documentation for calculation details\n    value: \"{{ .Values.producer.batchSize }}\"\n  - name: KAFKA_PRODUCER_LINGER_MS\n    # Producer linger time - allows batching multiple messages\n    # Default: 0 (send immediately) - results in one network call per message\n    # Current: {{ .Values.producer.lingerMs }}ms - balances latency vs throughput\n    # Trade-off: +{{ .Values.producer.lingerMs }}ms latency for 5-10x fewer network calls\n    # Monitor: record-send-rate and request-latency-avg in DataDog\n    value: \"{{ .Values.producer.lingerMs }}\"\n</code></pre> <p>In your <code>values.yaml</code> (dev.yaml, sandbox.yaml, prod.yaml):</p> <pre><code>producer:\n  batchSize: 1024  # 1KB - Conservative optimization for small messages\n  lingerMs: 5      # 5ms - Balances latency and throughput\n</code></pre>"},{"location":"produce-events/#code-samples","title":"Code samples","text":"<p>Info</p> <p>Expand the sections below to see producer code examples in Java and Ruby. To see the samples in context, please check out the <code>ves-event-bus-sample-code</code> repository (must be part of VA GitHub organization to view).</p> Java Producer <p><pre><code>package gov.va.eventbus.example;\n\nimport io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;\nimport io.confluent.kafka.serializers.KafkaAvroSerializer;\nimport io.confluent.kafka.serializers.KafkaAvroSerializerConfig;\nimport java.time.LocalDate;\nimport java.util.Properties;\nimport org.apache.avro.specific.SpecificRecord;\nimport org.apache.kafka.clients.CommonClientConfigs;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.common.config.SaslConfigs;\nimport org.apache.kafka.common.config.SslConfigs;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class TestProducer implements Runnable {\n    private static final Logger LOG = LoggerFactory.getLogger(TestProducer.class);\n\n    // Producer values\n    private static final String TOPIC = \"test\";\n    private static final String EB_BOOTSTRAP_SERVERS = System.getenv(\"EB_BOOTSTRAP_SERVERS\");\n    private static final String EB_SECURITY_PROTOCOL = System.getenv(\"EB_SECURITY_PROTOCOL\");\n    private static final String SCHEMA_REGISTRY_URL = System.getenv(\"SCHEMA_REGISTRY_URL\");\n    private static final String AWS_ROLE = System.getenv(\"AWS_ROLE\");\n\n    private final KafkaProducer&lt;Long, SpecificRecord&gt; producer;\n\n    public TestProducer() {\n        this.producer = createProducer();\n    }\n\n    @Override\n    public void run() {\n        try {\n            var sequenceNumber = 0;\n            while (true) {\n                sequenceNumber++;\n                // The messages in the topic adhere to a User schema\n                var user = User.newBuilder()\n                        .setName(\"Newbie\")\n                        .setCompany(\"Ad Hoc\")\n                        .setDateOfBirth(LocalDate.of(2000,7,26))\n                        .setSequenceNumber(sequenceNumber)\n                        .build();\n\n                // Create producer record\n                ProducerRecord&lt;Long, SpecificRecord&gt; producerRecord = new ProducerRecord&lt;&gt;(TOPIC, user);\n\n                // Send the record to the Kafka topic\n                producer.send(producerRecord);\n                Thread.sleep(1000);\n            }\n        } catch (final Exception e) {\n            LOG.error(\"An exception occurred while producing messages\", e);\n        } finally {\n            producer.close();\n        }\n    }\n\n    private KafkaProducer&lt;Long, SpecificRecord&gt; createProducer() {\n        final Properties props = new Properties();\n\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, EB_BOOTSTRAP_SERVERS);\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());\n        props.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG, SCHEMA_REGISTRY_URL);\n        props.put(KafkaAvroSerializerConfig.AUTO_REGISTER_SCHEMAS, false);\n        producerProps.put(KafkaAvroSerializerConfig.USE_LATEST_VERSION, true);\n        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, false);\n        // * See note below about SCHEMA_ID_FROM_EB\n        props.put(ProducerConfig.USE_SCHEMA_ID, &lt;SCHEMA_ID_FROM_EB&gt;); \n\n        // Use SASL_SSL in production but PLAINTEXT in local environment\n        // w/docker_compose\n        if (\"SASL_SSL\".equals(EB_SECURITY_PROTOCOL)) {\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, EB_SECURITY_PROTOCOL);\n            props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"/tmp/kafka.client.truststore.jks\");\n            props.put(SaslConfigs.SASL_MECHANISM, \"OAUTHBEARER\");\n            props.put(SaslConfigs.SASL_JAAS_CONFIG,\n                    \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required awsRoleArn=\\\"\"\n                            + AWS_ROLE // use the role name provided to you\n                            + \"\\\" awsStsRegion=\\\"us-gov-west-1\\\";\");\n            props.put(SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS,\n                    \"software.amazon.msk.auth.iam.IAMOAuthBearerLoginCallbackHandler\");\n            props.put(SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS,\n                    \"software.amazon.msk.auth.iam.IAMOAuthBearerLoginCallbackHandler\");\n        } else if (!\"PLAINTEXT\".equals(EB_SECURITY_PROTOCOL)) {\n            LOG.error(\"Unknown EB_SECURITY_PROTOCOL '{}'\", EB_SECURITY_PROTOCOL);\n        }\n\n        return new KafkaProducer&lt;&gt;(props);\n    }\n}\n</code></pre> *<code>SCHEMA_ID_FROM_EB</code>: Schema ID is an integer ID assigned by the Schema Registry. The Event Bus Team will provide you this value.  Reach out to them when you are ready.</p> Ruby Producer <pre><code>require 'logger'\nrequire 'rdkafka'\nrequire 'avro_turf/messaging'\nrequire_relative 'oauth_token_refresher'\n\nlogger = Logger.new(STDOUT)\n\n@producers = {}\n\ndef refresh_token(_config, producer_name)\n    producer = @producers[producer_name]\n    OAuthTokenRefresher.new.refresh_token(producer)\nend\n\nsecurity_protocol = ENV['SECURITY_PROTOCOL']\n\nproperties = {\n    'bootstrap.servers': ENV['KAFKA_HOST'],\n    'security.protocol': security_protocol,\n    'enable.idempotence': false\n}\n\nif 'SASL_SSL' == security_protocol.upcase\n    properties['sasl.mechanisms'] = 'OAUTHBEARER'\n    Rdkafka::Config.oauthbearer_token_refresh_callback = method(:refresh_token)\nend\n\nproducer = Rdkafka::Config.new(properties).producer(native_kafka_auto_start: false)\n@producers[producer.name] = producer\nproducer.start\n\navro = AvroTurf::Messaging.new(registry_url: ENV['SCHEMA_REGISTRY_URL'], registry_path_prefix: ENV['SCHEMA_REGISTRY_PATH_PREFIX'])\n\nlogger.info \"Running producer\"\nwhile true do\n    payload = avro.encode({\"name\"=&gt;\"John Smith\", \"appointment_time\"=&gt;\"#{DateTime.now}\"}, subject: 'appointments-value', version: 1)\n    delivery_handle = producer.produce(topic: \"appointments\", payload: payload)\n    delivery_handle.wait\n    sleep 15\nend\n</code></pre>"},{"location":"produce-events/#register-with-code-va","title":"Register with CODE VA","text":"<p>CODE VA (must be on VA network to view) is a software catalog that houses information about software entities from across VA. Once you have your producer application up and running, it's important to register with the catalog to ensure that both current and future consumers can discover your event and access its details.</p> <p>In Backstage, entities represent software components or resources that share a common data shape and semantics. While there are several built-in entities, we have specifically created a custom entity called \"Event\" for the Event Bus.</p> <p>To register your event with CODE VA:</p> <ol> <li>Create a file named <code>catalog-info.yaml</code> at the root of your source code repository.</li> <li>Populate the <code>catalog-info.yaml</code> file with an <code>Event</code> Backstage entity based on the template below.</li> <li>Once your <code>catalog-info.yaml</code> file has been committed it will be automatically processed and the event will be viewable on CODE VA (must be on the VA network to view) within a few hours. If you would like the event to display quicker, follow the Backstage documentation on the default method for adding entries to the catalog.</li> </ol>"},{"location":"produce-events/#event-template","title":"Event Template","text":"<pre><code>apiVersion: backstage.io/v1alpha1\nkind: Event\nmetadata:\n    name: event-name\n    description: Event description.\n    title: Event Name\n    links:\n    - url: https://sample-slack-link.com\n        title: Event Producer Slack Channel\n    - url: https://sample-link.com\n        title: Event Documentation\nspec:\n    type: event\n    lifecycle: production\n    domain: health\n    sourceSystems:\n    - systemName: Source System Name\n        teamName: Source System Owning Team\n        productOwner: Source System Product Owner\n    topics:\n    - topic: topic_name\n        environment: development\n    - topic: topic_name\n        environment: production\n    forwarders:\n    - systemName: Forwarding System\n        teamName: Forwarding System Owning Team\n    retention: 20\n</code></pre> <p>Here is some additional information on the individual fields:</p> <ul> <li>apiVersion [required]: This value must be set to <code>backstage.io/v1alpha1</code>.</li> <li>kind [required]: This value must be set to <code>Event</code>.</li> <li>metadata [required]: A structure that contains information about the entity itself. The <code>metadata</code> structure includes the following properties.<ul> <li>name [required]: A machine-readable name for the event. This value will be used in CODE VA urls, so it should be all lowercase and use hyphens as separators.    </li> <li>description [required]: A concise, high-level description of the event and the data it provides.</li> <li>title [required]: A human-readable representation of the <code>name</code> to be used in CODE VA user interfaces.</li> <li>links [optional]: A list of links related to the event. Each link consists of a <code>url</code> and a <code>title</code>.<ul> <li>url [required]: The external url that will be opened when the link is clicked.</li> <li>title [required]: Display text for the link.</li> </ul> </li> </ul> </li> <li>spec [required]: A structure that contains information about the event a producer will be emitting. The <code>spec</code> structure includes the following properties.<ul> <li>type [required]: This value must be set to <code>event</code>.</li> <li>lifecycle [required]: The current development status for the event. This value must be set to: <code>experimental</code>, <code>development</code>, <code>production</code>, or <code>deprecated</code>.</li> <li>removalDate [optional]: This property should only be set if <code>lifecycle</code> is set to <code>deprecated</code>. This property specifies the date that a deprecated event will be removed.</li> <li>domain [required]: The VA domain in which a particular event exists. Possible values might be: <code>claims status</code>, <code>health</code>, <code>appointments</code>, <code>benefits</code>, etc.</li> <li>sourceSystems [required]: An array of objects that contain information about the sources of this event. Each source system will contain the following fields.<ul> <li>systemName [required]: The name of the system that sources this event.</li> <li>teamName [required]: The name of the team that owns this system.</li> <li>productOwner [required]: OCTODE/VA PO embedded on the team that owns this system.</li> </ul> </li> <li>topics [required]: An array of objects that contain information about the Kafka topics these events will be published to. The example above shows a topic for the development and production environments. Each topic will contain the following fields.<ul> <li>topic [required]: The name of the topic.</li> <li>environment [required]: The environment this topic is available in. This value must be set to <code>development</code> or <code>production</code>.</li> </ul> </li> <li>forwarders [optional]: An array of objects that contain information about systems that forward this event. This property should be used if there is a system sitting in between the source data store and the Event Bus that mutates data before an event is published. Each forwarder will contain the following fields.<ul> <li>systemName [required]: The name of the system that forwards this event.</li> <li>teamName [required]: The name of the team that owns this forwarding system.</li> </ul> </li> <li>retention [optional]: This value represents the number of days that each event is retained for. It should be set to an integer. This property only needs to be set if your topic has a custom retention policy. If it is not set, the default of 7 days will be displayed.</li> </ul> </li> </ul> <p>The <code>catalog.yaml</code> file will be validated against this JSON schema (must be part of VA GitHub organization to view). The required <code>spec.schema</code>, <code>spec.schemaCompatibilityMode</code>, and <code>spec.topics.brokerAddresses</code> fields included in this JSON schema will be auto-populated and should not be included in the <code>catalog-info.yaml</code> file. The optional <code>spec.averageDailyEvents</code> field will also be auto-populated and should not be included in the <code>catalog-info.yaml</code> file.</p>"},{"location":"produce-events/#schema-evolution","title":"Schema Evolution","text":"<p>Eventually the schema of an event may evolve. After a new version of the schema is added to the schema registry, consumers will need to update their applications to handle the new schema version before producers update to produce events using the new schema. The Event Bus team will coordinate these updates with producers and consumers.</p>"},{"location":"produce-events/#logs","title":"Logs","text":"<p>Logs are stored within a LightHouse Delivery Infrastructure (LHDI) AWS S3 bucket. Only LHDI admins with AWS access can access this bucket and its content. Although producers and consumers will not have access to the S3 bucket directly, logs are available via LHDI's Datadog instance (must have VA LightHouseDI DataDog access to view).</p> <p>Datadog is a monitoring and analytics tool that is used within the VA. LHDI team members are admins within the Datadog space where the Event Bus metrics and logs are available. To request access to Datadog, complete the HelpDesk form on the ServiceNow Portal at ECC (Enterprise Command Center) Monitoring Services - your IT Service Portal (must be on the VA network to view).</p>"},{"location":"produce-events/#troubleshooting","title":"Troubleshooting","text":"<p>If you have questions or run into difficulties with any of these steps, please see our Contact and Support page.</p>"},{"location":"producer-onboarding/","title":"Producer Onboarding","text":"<p>Producer onboarding documentation is coming soon! If you need to reach out to us right now, feel free to file a github issue here</p>"},{"location":"terminology/","title":"Terminology","text":"<ul> <li> <p>Broker: (also called a server or node) orchestrates the storage and passing of messages. These are the machines that store and service the data.</p> </li> <li> <p>Consumer: the team, as well as the application, that subscribes to the topic and reads messages according to topic, partition and offset.</p> </li> <li> <p>Domain: a broad classification of events that corresponds to an area of business within the VA. One can think of a domain as a radio station that plays a certain type of music.</p> <ul> <li>For example, transition events, claims/benefits events, health events, life events, memorial events, and interaction events.</li> </ul> </li> <li> <p>Event: the type of business action that has occurred.</p> <ul> <li>For example, an appointment was created.</li> </ul> </li> <li> <p>Event (instance): a specific instance of a business action that has occurred  (note: event, message, and record can be seen interchangeably in various Kafka-related documentation).</p> <ul> <li>For example, an appointment was created for Karen to see an orthopedic specialist on 6/1/2023 at 10:30 am.</li> </ul> </li> <li> <p>MSK: Managed Streams for Kafka. The Enterprise Event Bus uses AWS Managed Streams for Kafka, a hosted version of Kafka that runs in the VA Enterprise Cloud.</p> </li> <li> <p>Partitions: storage units within a topic. They hold a subset of the records owned by the topic. This is a logical concept in Kafka.</p> <ul> <li>For example, the number of partitions will impact the distribution of your data. The Enterprise Event Bus team will provide guidance on allocating partitions for a topic during the onboarding process of a producer.</li> </ul> </li> <li> <p>Producer: the team, as well as the application that appends messages to the end of the topic. Messages are written to partitions on a round robin basis, or to a specific partition based on the message key.</p> </li> <li> <p>Topic: provides a destination for the storage of data. Each topic is split into one or more partitions. Topics are where events are durably stored; this is similar to how files are stored in a distributed file system. A topic can have many producers and many consumers. Continuing the music analogy from the definition of domain, a topic can be thought of as a radio channel that plays a single band.</p> <ul> <li>For example, appointment events can be streamed from the appointments topic.</li> </ul> </li> </ul> <p>Find additional terms and abbreviations on the Event Bus terminology page in Github (must be part of VA GitHub organization to view). </p>"},{"location":"use-events/","title":"Event Catalog","text":""},{"location":"use-events/#about-the-catalog","title":"About the Catalog","text":"<p>The Event Catalog is the single source of truth for all events currently published on the Event Bus. The Event Catalog lives on CODE VA (formerly called the Lighthouse Hub).</p> <p>Visit the CODE VA website (must be on the VA network to view): https://code.va.gov/ (click on Software Catalog in left nav and then Events in top nav to display Events list) or use this direct link.</p> <p>In the Event Catalog, you\u2019ll find the following information for every event:</p> <ul> <li>A brief overview of the event, including its: <ul> <li>description</li> <li>VA domain</li> <li>system of record</li> <li>topics</li> <li>expected throughput</li> <li>development lifecycle phase</li> <li>consuming systems</li> </ul> </li> <li>Schema: the overall structure of an event payload, described as an Avro schema. Read more about Avro on the Oracle website.</li> </ul> <p></p> <p></p>"},{"location":"use-events/#producing-and-consuming-events-from-the-catalog","title":"Producing and consuming events from the catalog","text":"<p>Producers add events to the catalog and are responsible for keeping them up to date. You can read more about what to expect as a producer on the Producing Events page.</p> <p>Consumers can use the catalog to browse all available events, and view details about each one such as payload structure and the system that produces it. If you find an event in the catalog that you\u2019d like to consume, follow the steps on the Consuming Events page.</p>"},{"location":"use-events/#event-catalog-governance","title":"Event Catalog governance","text":"<p>The Event Catalog uses Backstage as its framework for hosting and displaying entries. Entries are added to the catalog through the use of <code>catalog-info.yaml</code> files that contain information about the source code they represent. You can read more about <code>catalog-info.yaml</code> files in the official Backstage documentation. These entries can represent anything from an API endpoint to an overarching system. For the Event Catalog, all entries represent an event that is currently, or was previously, published to the Event Bus.</p> <p>If you\u2019re interested in producing events and publishing them to the Event Bus, we provide a template for getting started with adding your event to the catalog. You can read more about the template and the information that needs to be added to it on the Producing Events page.</p>"},{"location":"use-events/#having-trouble","title":"Having trouble?","text":"<p>If you find something wrong in our documentation, didn\u2019t find what you were looking for, or have a question or suggestion, please see our Contact and Support page.</p>"}]}